groups:
  - name: ai_social_media_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~'5..', job='backend'}[5m]) / rate(http_requests_total{job='backend'}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% which is above 5%"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job='backend'}[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s which is above 2s"

      - alert: LowDiskSpace
        expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space"
          description: "Disk usage is {{ $value }}% which is above 85%"

      - alert: HighMemoryUsage
        expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}% which is above 90%"

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100) > 95
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}% which is above 95%"

      - alert: DatabaseConnectionPoolExhausted
        expr: pg_stat_activity_count{datname='social_media_db'} > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Active database connections: {{ $value }}"

      - alert: AgentTasksFailing
        expr: increase(celery_task_failed_total{job='agent'}[10m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Agent tasks are failing"
          description: "{{ $value }} tasks failed in the last 10 minutes"

      - alert: BackendServiceDown
        expr: up{job='backend'} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Backend service is down"
          description: "Backend service has been down for more than 1 minute"

      - alert: AgentServiceDown
        expr: up{job='agent'} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Agent service is down"
          description: "Agent service has been down for more than 5 minutes"

      - alert: FrontendServiceDown
        expr: up{job='frontend'} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Frontend service is down"
          description: "Frontend service has been down for more than 2 minutes"
